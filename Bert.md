# Bert

Bert is google's new technique for NLP pre-training which called Bidirectional Encoder Representations from Transformers. 

## The powerful of Bert
- Bidirectional

- Bidirectional


####Bidirectional
Compare with privious model, Bert achieves the goal that changing sequential computation to bidirectional. It can help us to overcome the restriction in left-to-right architecture that every token can only attended to previous tokens in the self-attention layers of the Transformer([Vaswani et al.,2017](https://pdfs.semanticscholar.org/8656/df5ece8f482c717e8381cc114dee161f9a3f.pdf?_ga=2.91985625.233109167.1566939361-747780848.1566326833)). Also, Bert proposing mask language model to pre-train bidirectional representaion.
